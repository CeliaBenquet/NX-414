{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celia/miniconda3/envs/neuro/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "import torch.nn as nn\n",
    "from torch.optim import NAdam\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score, explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_it_data, visualize_img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import gdown\n",
    "import os\n",
    "\n",
    "output = \"IT_data.h5\"\n",
    "\n",
    "if not os.path.exists(output):\n",
    "    url = \"https://drive.google.com/file/d/1s6caFNRpyR9m7ZM6XEv_e8mcXT3_PnHS/view?usp=share_link\"\n",
    "    gdown.download(url, output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"\"\n",
    "\n",
    "(\n",
    "    stimulus_train,\n",
    "    stimulus_val,\n",
    "    stimulus_test,\n",
    "    objects_train,\n",
    "    objects_val,\n",
    "    objects_test,\n",
    "    spikes_train,\n",
    "    spikes_val,\n",
    ") = load_it_data(path_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model,scheduler, train_loader, validation_dataloader, criterion, optimizer, num_epochs=10, device='cuda', max_patience=20):\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    best_loss = 1e3\n",
    "    patience = 0\n",
    "    with tqdm(total=num_epochs, desc=f\"Epoch 0/{num_epochs}\") as pbar:\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs), desc=f\"Training epochs\"):\n",
    "            running_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if not scheduler is None:\n",
    "                    scheduler.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in validation_dataloader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            val_loss = val_loss / len(validation_dataloader.dataset)\n",
    "            if val_loss<best_loss:\n",
    "                best_model_state_dict = {k:v.detach().to('cpu') for k, v in model.state_dict().items()}\n",
    "                best_model_state_dict = collections.OrderedDict(best_model_state_dict)\n",
    "                best_loss = val_loss\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience+=1\n",
    "\n",
    "            if patience == max_patience:\n",
    "                break\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            pbar.set_postfix(train_loss=epoch_loss , val_loss=val_loss, patience=patience)\n",
    "            pbar.update(1)\n",
    "            pbar.refresh() \n",
    "            \n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, validation_dataloader, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for inputs, labels in validation_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        predictions.append(model(inputs).detach().cpu())\n",
    "        true_labels.append(labels)\n",
    "    y_pred = torch.cat(predictions, dim=0).numpy()\n",
    "    y_true = torch.cat(true_labels, dim=0).numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    explained_variance = explained_variance_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(\"R2:\", r2)\n",
    "    print(\"Explained Variance:\", explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\"\"\"\n",
    "\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data driven approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our best-performing model: pretrained Resnet-152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTensorDataset(\n",
    "    (\n",
    "        torch.Tensor(stimulus_train),\n",
    "        torch.Tensor(spikes_train),\n",
    "    ),\n",
    "    transform=ResNet152_Weights.IMAGENET1K_V1.transforms(),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = CustomTensorDataset(\n",
    "    (torch.Tensor(stimulus_val), torch.Tensor(spikes_val)),\n",
    "    transform=ResNet152_Weights.IMAGENET1K_V1.transforms(),\n",
    ")\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:  64%|██████▎   | 127/200 [51:30<29:36, 24.33s/it]atience=19, train_loss=0.00773, val_loss=0.0681]\n",
      "Epoch 127/200:  64%|██████▎   | 127/200 [51:30<29:36, 24.33s/it, patience=19, train_loss=0.00773, val_loss=0.0681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "MSE: 0.06672779470682144\n",
      "R2: 0.43175606231408487\n",
      "Explained Variance: 0.452819168922447\n"
     ]
    }
   ],
   "source": [
    "model = resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)\n",
    "\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "custom_layer = CustomLayer(in_features, spikes_train.shape[1])\n",
    "model.fc = custom_layer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 200\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = NAdam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    max_lr=lr,\n",
    "    total_steps=num_epochs * len(train_loader),\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy=\"linear\",\n",
    "    cycle_momentum=False,\n",
    "    div_factor=1e2,\n",
    "    final_div_factor=0.1,\n",
    ")\n",
    "\n",
    "model = train_model(\n",
    "    model, scheduler, train_loader, validation_loader, criterion, optimizer, num_epochs=num_epochs\n",
    ")\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet152_best_model.pth\")\n",
    "\n",
    "model.eval()\n",
    "print(\"##################\")\n",
    "evaluate(model, validation_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
